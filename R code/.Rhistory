for(j in 1:10){
m_0 <- array(0,dim = n_train)
for(i in 1:n_train){
m_0[i] <- to_train[i+lookback-1+delay,j+1]
}
assign(paste0("tw",j),m_0)
}
#validation
val <- array(0,dim = c(n_val,lookback/step,13))
vw.5 <- array(0,dim = n_val)
vw12 <- array(0,dim = n_val)
for(i in 1:n_val){
val[i,,] <- to_train[seq(i+n_train,i+n_train+lookback-1,
length.out = lookback/step),]
vw.5[i] <- to_train[i+n_train+lookback-1+delay,1]
vw12[i] <- to_train[i+n_train+lookback-1+delay,12]
}
for(j in 1:10){
m_0 <- array(0,dim = n_val)
for(i in 1:n_val){
m_0[i] <- to_train[i+n_train+lookback-1+delay,j+1]
}
assign(paste0("vw",j),m_0)
}
#test
test <- array(0,dim = c(n_test,lookback/step,13))
tew.5 <- array(0,dim = n_test)
tew12 <- array(0,dim = n_test)
for(i in 1:n_test){
test[i,,] <- to_train[seq(i+n_train+n_val,i+n_train+n_val+lookback-1,
length.out = lookback/step),]
tew.5[i] <- to_train[i+n_train+n_val+lookback-1+delay,1]
tew12[i] <- to_train[i+n_train+n_val+lookback-1+delay,12]
}
for(j in 1:10){
m_0 <- array(0,dim = n_test)
for(i in 1:n_test){
m_0[i] <- to_train[i+n_train+n_val+lookback-1+delay,j+1]
}
assign(paste0("tew",j),m_0)
}
}
mark <- c(".5",1,2,3,4,5,6,7,8,9,10,12)
for(kk in 1:12){
matr <- matrix(0,ncol = 9,nrow = 10)
colnames(matr) <- rep(c("MAE","RMSE","NSE"),3)
for(i in 1:10){
input <- layer_input(shape = c(lookback/step,13))
output <- input %>% layer_separable_conv_1d(filters = 64,kernel_size = 5,
activation = "relu") %>%
layer_lstm(units = 32) %>%
layer_dense(units = 1)
model_scnnlstm <- keras_model(input,output)
model_scnnlstm %>% compile(optimizer="nadam",
loss="mae")
callback <- list(
callback_model_checkpoint(monitor = "val_loss",
save_best_only = T,
filepath = "watertemp-SBAS.h5"),
callback_reduce_lr_on_plateau(monitor = "val_loss",
factor = 0.2,
patience = 5)
)
model_scnnlstm %>% fit(train,get(paste0("tw",mark[kk])),
validation_data=list(val,get(paste0("vw",mark[kk]))),
epoch=15,
batch_size=32,
callbacks=callback,
verbose=2)
model_scnnlstm <- load_model_hdf5("watertemp-SBAS.h5")
save_model_hdf5(model_scnnlstm,paste0("CSNN-LSTM-SBAS_",mark[kk],"_",i,".h5"))
pre <- model_scnnlstm %>% predict(test)*sd[kk]+mean[kk]
ori <- get(paste0("tew",mark[kk]))*sd[kk]+mean[kk]
test_mae <- mean(abs(pre[,1]-ori))
test_rmse <- sqrt(mean((pre[,1]-ori)^2))
test_nse <- 1-sum((pre[,1]-ori)^2)/sum((ori-mean(ori))^2)
pre <- model_scnnlstm %>% predict(val)*sd[kk]+mean[kk]
ori <- get(paste0("vw",mark[kk]))*sd[kk]+mean[kk]
val_mae <- mean(abs(pre[,1]-ori))
val_rmse <- sqrt(mean((pre[,1]-ori)^2))
val_nse <- 1-sum((pre[,1]-ori)^2)/sum((ori-mean(ori))^2)
pre <- model_scnnlstm %>% predict(train)*sd[kk]+mean[kk]
ori <- get(paste0("tw",mark[kk]))*sd[kk]+mean[kk]
train_mae <- mean(abs(pre[,1]-ori))
train_rmse <- sqrt(mean((pre[,1]-ori)^2))
train_nse <- 1-sum((pre[,1]-ori)^2)/sum((ori-mean(ori))^2)
matr[i,] <- c(train_mae,train_rmse,train_nse,
val_mae,val_rmse,val_nse,
test_mae,test_rmse,test_nse)
print(c(kk,i))
}
assign(paste0("CSNN-SBAS_",mark[kk]),matr)
write.csv(get(paste0("CSNN-SBAS_",mark[kk])),paste0("CSNN-SBAS_",mark[kk],".csv"))
}
View(GA_XGB)
View(GA_XGB)
View(callback)
View(callback)
View(callback)
View(callback)
#A code file for prediction by CNN-LSTM considering 3 variables
library(keras)
ori_data <- read.csv("Blel_2008_2009_2010_2011.csv")
ori_data <- ori_data[-1,-1]
ori_data <- ori_data[,c(1,12,13)]
colnames(ori_data) <- c("WT.5","WT12","AT")
ori_data <- na.omit(ori_data)
for(i in 1:3){
ori_data[,i] <- as.numeric(ori_data[,i])
}
lookback <- 144
step <- 1
delay <- 12
n_sample <- dim(ori_data)[1]
n_train <- round(n_sample*0.5)
n_val <- round(n_sample*0.25)
n_test <- n_sample-n_train-n_val-lookback-delay+1
mean <- apply(ori_data[1:n_train,],2,mean)
sd <- apply(ori_data[1:n_train,],2,sd)
to_train <- scale(ori_data,center = mean,scale = sd)
#Dividing dataset
{
#training
train <- array(0,dim = c(n_train,lookback/step,3))
tw.5 <- array(0,dim = n_train)
for(i in 1:n_train){
train[i,,] <- to_train[seq(i,i+lookback-1,
length.out = lookback/step),]
tw.5[i] <- to_train[i+lookback-1+delay,1]
}
#validation
val <- array(0,dim = c(n_val,lookback/step,3))
vw.5 <- array(0,dim = n_val)
for(i in 1:n_val){
val[i,,] <- to_train[seq(i+n_train,i+n_train+lookback-1,
length.out = lookback/step),]
vw.5[i] <- to_train[i+n_train+lookback-1+delay,1]
}
#test
test <- array(0,dim = c(n_test,lookback/step,3))
tew.5 <- array(0,dim = n_test)
for(i in 1:n_test){
test[i,,] <- to_train[seq(i+n_train+n_val,i+n_train+n_val+lookback-1,
length.out = lookback/step),]
tew.5[i] <- to_train[i+n_train+n_val+lookback-1+delay,1]
}
}
input <- layer_input(shape = c(lookback/step,3))
output <- input %>% layer_conv_1d(filters = 64,kernel_size = 5,
activation = "relu") %>%
layer_lstm(units = 32) %>%
layer_dense(units = 1)
model_cnnlstm <- keras_model(input,output)
model_cnnlstm %>% compile(optimizer="nadam",
loss="mae")
callback <- list(
callback_model_checkpoint(monitor = "val_loss",
save_best_only = T,
filepath = "watertemp.h5"),
callback_reduce_lr_on_plateau(monitor = "val_loss",
factor = 0.2,
patience = 4)
)
model_cnnlstm %>% fit(train,tw.5,
validation_data=list(val,vw.5),
epoch=15,
batch_size=32,
callbacks=callback,
verbose=2)
# Import necessary library
library(keras)
# Read the CSV file
ori_data <- read.csv("Blel_2008_2009_2010_2011.csv")
ori_data <- ori_data[-1,-1]  # Remove the first row and first column
ori_data <- ori_data[,c(1,12,13)]  # Select columns WT.5, WT12, and AT
colnames(ori_data) <- c("WT.5","WT12","AT")  # Rename columns
# Remove missing values
ori_data <- na.omit(ori_data)
# Convert data to numeric format
for(i in 1:3){
ori_data[,i] <- as.numeric(ori_data[,i])
}
# Set parameters for sliding window
lookback <- 144
step <- 1
delay <- 12
n_sample <- dim(ori_data)[1]  # Total number of samples
n_train <- round(n_sample*0.5)  # Training set size
n_val <- round(n_sample*0.25)  # Validation set size
n_test <- n_sample-n_train-n_val-lookback-delay+1  # Test set size
# Standardize the data using training set mean and standard deviation
mean <- apply(ori_data[1:n_train,],2,mean)
sd <- apply(ori_data[1:n_train,],2,sd)
to_train <- scale(ori_data,center = mean,scale = sd)
# Divide dataset
{
# Training set
train <- array(0,dim = c(n_train,lookback/step,3))  # Initialize input array
tw.5 <- array(0,dim = n_train)  # Initialize target array
for(i in 1:n_train){
train[i,,] <- to_train[seq(i,i+lookback-1,
length.out = lookback/step),]
tw.5[i] <- to_train[i+lookback-1+delay,1]  # Target value
}
# Validation set
val <- array(0,dim = c(n_val,lookback/step,3))  # Initialize input array
vw.5 <- array(0,dim = n_val)  # Initialize target array
for(i in 1:n_val){
val[i,,] <- to_train[seq(i+n_train,i+n_train+lookback-1,
length.out = lookback/step),]
vw.5[i] <- to_train[i+n_train+lookback-1+delay,1]  # Target value
}
# Test set
test <- array(0,dim = c(n_test,lookback/step,3))  # Initialize input array
tew.5 <- array(0,dim = n_test)  # Initialize target array
for(i in 1:n_test){
test[i,,] <- to_train[seq(i+n_train+n_val,i+n_train+n_val+lookback-1,
length.out = lookback/step),]
tew.5[i] <- to_train[i+n_train+n_val+lookback-1+delay,1]  # Target value
}
}
# Define the CNN-LSTM model
input <- layer_input(shape = c(lookback/step,3))  # Input layer
output <- input %>% layer_conv_1d(filters = 64,kernel_size = 5,
activation = "relu") %>%  # Convolutional layer
layer_lstm(units = 32) %>%  # LSTM layer
layer_dense(units = 1)  # Dense layer for output
model_cnnlstm <- keras_model(input,output)
model_cnnlstm %>% compile(optimizer="nadam",  # Compile the model
loss="mae")
callback <- list(
callback_model_checkpoint(monitor = "val_loss",  # Save the best model
save_best_only = T,
filepath = "watertemp.h5"),
callback_reduce_lr_on_plateau(monitor = "val_loss",  # Reduce learning rate on plateau
factor = 0.2,
patience = 4)
)
# Train the model
model_cnnlstm %>% fit(train,tw.5,
validation_data=list(val,vw.5),
epoch=15,
batch_size=32,
callbacks=callback,
verbose=2)
#setwd("E:\\Rdatafile\\prediction of lake temp\\CCNN")
ori_data <- read.csv("Blel_2008_2009_2010_2011.csv")
ori_data <- ori_data[-1,-1]
ori_data <- ori_data[,-c(14,15)]
colnames(ori_data) <- c("WT.5","WT1","WT2","WT3","WT4",
"wT5","WT6","WT7","WT8","WT9","WT10",
"WT12","AT")
ori_data <- na.omit(ori_data)
for(i in 1:12){
ori_data[,i] <- as.numeric(ori_data[,i])
}
lookback <- 144
step <- 1
delay <- 12
n_sample <- dim(ori_data)[1]
n_train <- round(n_sample*0.5)
n_val <- round(n_sample*0.25)
n_test <- n_sample-n_train-n_val-lookback-delay+1
mean <- apply(ori_data[1:n_train,],2,mean)
sd <- apply(ori_data[1:n_train,],2,sd)
to_train <- scale(ori_data,center = mean,scale = sd)
{
#training
train <- array(0,dim = c(n_train,lookback/step,13))
tw.5 <- array(0,dim = n_train)
tw12 <- array(0,dim = n_train)
for(i in 1:n_train){
train[i,,] <- to_train[seq(i,i+lookback-1,
length.out = lookback/step),]
tw.5[i] <- to_train[i+lookback-1+delay,1]
tw12[i] <- to_train[i+lookback-1+delay,12]
}
for(j in 1:10){
m_0 <- array(0,dim = n_train)
for(i in 1:n_train){
m_0[i] <- to_train[i+lookback-1+delay,j+1]
}
assign(paste0("tw",j),m_0)
}
#validation
val <- array(0,dim = c(n_val,lookback/step,13))
vw.5 <- array(0,dim = n_val)
vw12 <- array(0,dim = n_val)
for(i in 1:n_val){
val[i,,] <- to_train[seq(i+n_train,i+n_train+lookback-1,
length.out = lookback/step),]
vw.5[i] <- to_train[i+n_train+lookback-1+delay,1]
vw12[i] <- to_train[i+n_train+lookback-1+delay,12]
}
for(j in 1:10){
m_0 <- array(0,dim = n_val)
for(i in 1:n_val){
m_0[i] <- to_train[i+n_train+lookback-1+delay,j+1]
}
assign(paste0("vw",j),m_0)
}
#test
test <- array(0,dim = c(n_test,lookback/step,13))
tew.5 <- array(0,dim = n_test)
tew12 <- array(0,dim = n_test)
for(i in 1:n_test){
test[i,,] <- to_train[seq(i+n_train+n_val,i+n_train+n_val+lookback-1,
length.out = lookback/step),]
tew.5[i] <- to_train[i+n_train+n_val+lookback-1+delay,1]
tew12[i] <- to_train[i+n_train+n_val+lookback-1+delay,12]
}
for(j in 1:10){
m_0 <- array(0,dim = n_test)
for(i in 1:n_test){
m_0[i] <- to_train[i+n_train+n_val+lookback-1+delay,j+1]
}
assign(paste0("tew",j),m_0)
}
}
library(keras)
mark <- c(".5",1,2,3,4,5,6,7,8,9,10,12)
for(kk in 9:12){
matr <- matrix(0,ncol = 9,nrow = 10)
colnames(matr) <- rep(c("MAE","RMSE","NSE"),3)
for(i in 1:10){
input <- layer_input(shape = c(lookback/step,13))
output <- input %>% layer_conv_1d(filters = 64,kernel_size = 5,
activation = "relu") %>%
layer_lstm(units = 32) %>%
layer_dense(units = 1)
model_lstm <- keras_model(input,output)
model_lstm %>% compile(optimizer="nadam",
loss="mae")
callback <- list(
callback_model_checkpoint(monitor = "val_loss",
save_best_only = T,
filepath = "watertemp.h5"),
callback_reduce_lr_on_plateau(monitor = "val_loss",
factor = 0.2,
patience = 4)
)
model_lstm %>% fit(train,get(paste0("tw",mark[kk])),
validation_data=list(val,get(paste0("vw",mark[kk]))),
epoch=15,
batch_size=32,
callbacks=callback,
verbose=2)
model_lstm <- load_model_hdf5("watertemp.h5")
save_model_hdf5(model_lstm,paste0("CCNN-LSTM_",mark[kk],"_",i,".h5"))
pre <- model_lstm %>% predict(test)*sd[kk]+mean[kk]
ori <- get(paste0("tew",mark[kk]))*sd[kk]+mean[kk]
test_mae <- mean(abs(pre[,1]-ori))
test_rmse <- sqrt(mean((pre[,1]-ori)^2))
test_nse <- 1-sum((pre[,1]-ori)^2)/sum((ori-mean(ori))^2)
pre <- model_lstm %>% predict(val)*sd[kk]+mean[kk]
ori <- get(paste0("vw",mark[kk]))*sd[kk]+mean[kk]
val_mae <- mean(abs(pre[,1]-ori))
val_rmse <- sqrt(mean((pre[,1]-ori)^2))
val_nse <- 1-sum((pre[,1]-ori)^2)/sum((ori-mean(ori))^2)
pre <- model_lstm %>% predict(train)*sd[kk]+mean[kk]
ori <- get(paste0("tw",mark[kk]))*sd[kk]+mean[kk]
train_mae <- mean(abs(pre[,1]-ori))
train_rmse <- sqrt(mean((pre[,1]-ori)^2))
train_nse <- 1-sum((pre[,1]-ori)^2)/sum((ori-mean(ori))^2)
matr[i,] <- c(train_mae,train_rmse,train_nse,
val_mae,val_rmse,val_nse,
test_mae,test_rmse,test_nse)
print(c(kk,i))
}
assign(paste0("CCNN_",mark[kk]),matr)
write.csv(get(paste0("CCNN_",mark[kk])),paste0("CCNN_",mark[kk],".csv"))
}
library(keras)
ori_data <- read.csv("Blel_2008_2009_2010_2011.csv")
ori_data <- ori_data[-1,-1]
ori_data <- ori_data[,c(1,12,13)]
colnames(ori_data) <- c("WT.5","WT12","AT")
ori_data <- na.omit(ori_data)
for(i in 1:3){
ori_data[,i] <- as.numeric(ori_data[,i])
}
lookback <- 144
step <- 1
delay <- 12
n_sample <- dim(ori_data)[1]
n_train <- round(n_sample*0.5)
n_val <- round(n_sample*0.25)
n_test <- n_sample-n_train-n_val-lookback-delay+1
mean <- apply(ori_data[1:n_train,],2,mean)
sd <- apply(ori_data[1:n_train,],2,sd)
to_train <- scale(ori_data,center = mean,scale = sd)
# Dividing dataset
{
#training
train <- array(0,dim = c(n_train,lookback/step,3))
tw.5 <- array(0,dim = n_train)
for(i in 1:n_train){
train[i,,] <- to_train[seq(i,i+lookback-1,
length.out = lookback/step),]
tw.5[i] <- to_train[i+lookback-1+delay,1]
}
#validation
val <- array(0,dim = c(n_val,lookback/step,3))
vw.5 <- array(0,dim = n_val)
for(i in 1:n_val){
val[i,,] <- to_train[seq(i+n_train,i+n_train+lookback-1,
length.out = lookback/step),]
vw.5[i] <- to_train[i+n_train+lookback-1+delay,1]
}
#test
test <- array(0,dim = c(n_test,lookback/step,3))
tew.5 <- array(0,dim = n_test)
for(i in 1:n_test){
test[i,,] <- to_train[seq(i+n_train+n_val,i+n_train+n_val+lookback-1,
length.out = lookback/step),]
tew.5[i] <- to_train[i+n_train+n_val+lookback-1+delay,1]
}
}
# Build LSTM model
input <- layer_input(shape = c(lookback/step,3))
output <- input %>%
layer_lstm(units = 32) %>%
layer_dense(units = 1)
model_lstm <- keras_model(input, output)
model_lstm %>% compile(optimizer = "nadam", loss = "mae")
# Callbacks
callback <- list(
callback_model_checkpoint(monitor = "val_loss",
save_best_only = TRUE,
filepath = "watertemp_lstm.h5"),
callback_reduce_lr_on_plateau(monitor = "val_loss",
factor = 0.2,
patience = 5)
)
# Training LSTM model
model_lstm %>% fit(train, tw.5,
validation_data = list(val, vw.5),
epochs = 15,
batch_size = 32,
callbacks = callback,
verbose = 2)
load("C:/Users/13640/Desktop/D论文1/JH origin paper - 提交版/JH code/XGB-13variables-10/XGB5.RData")
library(xgboost)
library(mcga)
# Read and modify data
ori_data <- read.csv("Blel_2008_2009_2010_2011.csv")
ori_data <- ori_data[-1, -1]  # Remove the first row and first column
ori_data <- ori_data[, c(1, 12, 13)]  # Keep only "WT.5", "WT12", "AT"
colnames(ori_data) <- c("WT.5", "WT12", "AT")
# Handle missing values
ori_data <- na.omit(ori_data)
for(i in 1:3){
ori_data[, i] <- as.numeric(ori_data[, i])
}
# Set parameters
lookback <- 144
step <- 6
delay <- 12
n_sample <- dim(ori_data)[1]
n_train <- round(n_sample * 0.5)
n_val <- round(n_sample * 0.25)
n_test <- n_sample - n_train - n_val - lookback - delay + 1
# Standardization
mean <- apply(ori_data[1:n_train,], 2, mean)
sd <- apply(ori_data[1:n_train,], 2, sd)
to_train <- scale(ori_data, center = mean, scale = sd)
# Build training, validation, and test sets
{
# Training set
train <- array(0, dim = c(n_train, lookback / step * 3))
twp5 <- array(0, dim = n_train)  # Label for WT.5
tw12 <- array(0, dim = n_train)  # Label for WT12
twat <- array(0, dim = n_train)  # Label for AT
for(i in 1:n_train){
train[i,] <- c(to_train[seq(i, i + lookback - 1, length.out = lookback / step), ])
twp5[i] <- to_train[i + lookback - 1 + delay, 1]  # WT.5
tw12[i] <- to_train[i + lookback - 1 + delay, 2]  # WT12
twat[i] <- to_train[i + lookback - 1 + delay, 3]  # AT
}
# Validation set
val <- array(0, dim = c(n_val, lookback / step * 3))
vwp5 <- array(0, dim = n_val)  # Label for WT.5
vw12 <- array(0, dim = n_val)  # Label for WT12
vwat <- array(0, dim = n_val)  # Label for AT
for(i in 1:n_val){
val[i,] <- c(to_train[seq(i + n_train, i + n_train + lookback - 1, length.out = lookback / step), ])
vwp5[i] <- to_train[i + n_train + lookback - 1 + delay, 1]  # WT.5
vw12[i] <- to_train[i + n_train + lookback - 1 + delay, 2]  # WT12
vwat[i] <- to_train[i + n_train + lookback - 1 + delay, 3]  # AT
}
# Test set
test <- array(0, dim = c(n_test, lookback / step * 3))
tewp5 <- array(0, dim = n_test)  # Label for WT.5
tew12 <- array(0, dim = n_test)  # Label for WT12
tewat <- array(0, dim = n_test)  # Label for AT
for(i in 1:n_test){
test[i,] <- c(to_train[seq(i + n_train + n_val, i + n_train + n_val + lookback - 1, length.out = lookback / step), ])
tewp5[i] <- to_train[i + n_train + n_val + lookback - 1 + delay, 1]  # WT.5
tew12[i] <- to_train[i + n_train + n_val + lookback - 1 + delay, 2]  # WT12
tewat[i] <- to_train[i + n_train + n_val + lookback - 1 + delay, 3]  # AT
}
}
# Fitness function for GA
ajfun <- function(x, train, twp5, val, vwp5){
xgb <- xgboost(data = train, label = twp5, verbose = 0,
nrounds = ceiling(x[1]), max_depth = ceiling(x[2]))
pre_val <- predict(xgb, val)
return(-mean((pre_val - vwp5)^2))
}
# Optimize hyperparameters using Genetic Algorithm (GA)
GA_XGB <- mcga2(fitness = ajfun, min = c(3, 6), max = c(40, 25),
maxiter = 30, popSize = 20, train = train, twp5 = twp5, val = val, vwp5 = vwp5)
